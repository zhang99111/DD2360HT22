{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhang99111/DD2360HT22/blob/main/hw_3/hw3_ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Next, we write a native CUDA code and save it as 'lab3_ex1.cu'\n"
      ],
      "metadata": {
        "id": "vsbr4brQH6v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lab3_ex1.cu\n",
        "#include <stdio.h>\n",
        "#include <sys/time.h>\n",
        "#include <stdlib.h>\n",
        "#include <random>\n",
        "#define DataType double\n",
        "\n",
        "__global__ void vecAdd(DataType *in1, DataType *in2, DataType *out, int len) {\n",
        "  //@@ Insert code to implement vector addition here\n",
        "  int idx=0; \n",
        "  idx=threadIdx.x+blockDim.x * blockIdx.x;\n",
        "  if(idx<len){\n",
        "    out[idx]=in1[idx]+in2[idx];\n",
        "  }\n",
        "\n",
        "}\n",
        "double cpuSecond() {\n",
        "   struct timeval tp;\n",
        "   gettimeofday(&tp,NULL);\n",
        "   return ((double)tp.tv_sec + (double)tp.tv_usec*1.e-6);\n",
        "}\n",
        "//@@ Insert code to implement timer start\n",
        "\n",
        "//@@ Insert code to implement timer stop\n",
        "\n",
        "double stopTimer(double starttime) {\n",
        "   struct timeval tp;\n",
        "   gettimeofday(&tp,NULL);\n",
        "   return ((double)tp.tv_sec + ((double)tp.tv_usec*1.e-6)-starttime);\n",
        "}\n",
        "int main(int argc, char **argv) {\n",
        "  \n",
        "  int inputLength;\n",
        "  DataType *hostInput1;\n",
        "  DataType *hostInput2;\n",
        "  DataType *hostOutput;\n",
        "  DataType *resultRef;\n",
        "  DataType *deviceInput1;\n",
        "  DataType *deviceInput2;\n",
        "  DataType *deviceOutput;\n",
        "\n",
        "  //@@ Insert code below to read in inputLength from args\n",
        "  \n",
        "  inputLength=atoi(argv[1]);\n",
        "  printf(\"The input length is %d\\n\", inputLength);\n",
        "  \n",
        "  //@@ Insert code below to allocate Host memory for input and output\n",
        "  \n",
        "  hostInput1 = (DataType*)malloc(inputLength*sizeof(*hostInput1));\n",
        "  hostInput2 = (DataType*)malloc(inputLength*sizeof(*hostInput2));\n",
        "  hostOutput = (DataType*)malloc(inputLength*sizeof(*hostOutput));\n",
        "  //@@ Insert code below to initialize hostInput1 and hostInput2 to random numbers, and create reference result in CPU\n",
        "  std::uniform_real_distribution<DataType> distribution(0.0, 1.0);\n",
        "  std::default_random_engine gen(1145);\n",
        "  for (DataType *ptr : {hostInput1, hostInput2}) {\n",
        "      for (int i=0; i<inputLength; ++i){\n",
        "          ptr[i] = distribution(gen);\n",
        "      }\n",
        "  }\n",
        "  resultRef = (DataType*)malloc(inputLength*sizeof(DataType));\n",
        "  double starttime=cpuSecond();\n",
        "  for(int i=0; i<inputLength; i++){\n",
        "    resultRef[i]=hostInput1[i]+hostInput2[i];\n",
        "  }\n",
        "  double stoptime_cpu=stopTimer(starttime);\n",
        "\n",
        "  //@@ Insert code below to allocate GPU memory here\n",
        "  cudaMalloc(&deviceInput1, inputLength*sizeof(hostInput1));\n",
        "  cudaMalloc(&deviceInput2, inputLength*sizeof(hostInput2));\n",
        "  cudaMalloc(&deviceOutput, inputLength*sizeof(hostOutput));\n",
        "\n",
        "  //@@ Insert code to below to Copy memory to the GPU here\n",
        "  double starttime_memory=cpuSecond();\n",
        "  cudaMemcpy(deviceInput1, hostInput1, inputLength*sizeof(DataType), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(deviceInput2, hostInput2, inputLength*sizeof(DataType), cudaMemcpyHostToDevice);\n",
        "  cudaDeviceSynchronize();\n",
        "  double stoptime_memory=stopTimer(starttime_memory);\n",
        "\n",
        "   int Db =1024;\n",
        "   int Dg = (inputLength + Db - 1) / Db;\n",
        "  //@@ Initialize the 1D grid and block dimensions here\n",
        " \n",
        "  \n",
        "  //@@ Launch the GPU Kernel here\n",
        "  double starttime_gpu=cpuSecond();\n",
        "  vecAdd<<<Dg,Db>>>(deviceInput1,deviceInput2,deviceOutput,inputLength);\n",
        "  cudaDeviceSynchronize();\n",
        "  double stoptime_gpu=stopTimer(starttime_gpu);\n",
        "  double starttime_memory_2=cpuSecond();\n",
        "\n",
        "  //@@ Copy the GPU memory back to the CPU here\n",
        "  cudaMemcpy(hostOutput, deviceOutput, inputLength*sizeof(DataType), cudaMemcpyDeviceToHost);\n",
        "  cudaDeviceSynchronize();\n",
        "  double stoptimer_memory_2=stopTimer(starttime_memory_2);\n",
        "\n",
        "\n",
        "  //@@ Insert code below to compare the output with the reference\n",
        "  for(int i=0;i<inputLength;i++)\n",
        "  {\n",
        "    if(resultRef[i] != hostOutput[i]&& abs(resultRef[i]-hostOutput[i])>0.0001){\n",
        "      printf(\"error counted:%f\",abs(resultRef[i]-hostOutput[i]));\n",
        "      return 0;\n",
        "    }\n",
        "  }\n",
        "  printf(\"cost Host->Device: %f - cost Device->Host: %f\\n\",stoptime_memory,stoptimer_memory_2);\n",
        "  printf(\"CPU cost: %f - GPU cost: %f\\n\",stoptime_cpu,stoptime_gpu);\n",
        "  //@@ Free the GPU memory here\n",
        "  cudaFree(deviceInput1);\n",
        "  cudaFree(deviceInput2);\n",
        "  cudaFree(deviceOutput);\n",
        "\n",
        "\n",
        "  //@@ Free the CPU memory here\n",
        "\n",
        "  free(hostInput1);\n",
        "  free(hostInput2);\n",
        "  free(hostOutput);\n",
        "\n",
        "  return 0;\n",
        "\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0S5AUrl4eI8",
        "outputId": "e4a5c180-6c58-4e4a-860d-2ac81925c7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting lab3_ex1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We compile the saved cuda code using nvcc compiler"
      ],
      "metadata": {
        "id": "TqdaBa9wIICn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 ./lab3_ex1.cu -o lab3_ex1\n",
        "\n"
      ],
      "metadata": {
        "id": "TvYps9NQ40NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally, we execute the binary of the compiled code"
      ],
      "metadata": {
        "id": "SUALHJy9IPvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex1 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6L-jFBt9dI9",
        "outputId": "9202c719-4466-4ad2-96e1-ed31bd5ecf83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 1024\n",
            "cost Host->Device: 0.000096 - cost Device->Host: 0.000023\n",
            "CPU cost: 0.000006 - GPU cost: 0.000062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda-11/bin/nv-nsight-cu-cli ./lab3_ex1 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfrW5fWte3ue",
        "outputId": "ada1c780-de68-4a2b-cc8a-6d5a0ffabac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 1024\n",
            "==PROF== Connected to process 470 (/content/lab3_ex1)\n",
            "==PROF== Profiling \"vecAdd\" - 1: 0%....50%....100% - 8 passes\n",
            "cost Host->Device: 0.000068 - cost Device->Host: 0.000068\n",
            "CPU cost: 0.000007 - GPU cost: 0.757928\n",
            "==PROF== Disconnected from process 470\n",
            "[470] lab3_ex1@127.0.0.1\n",
            "  vecAdd(double*, double*, double*, int), 2022-Dec-15 11:53:53, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.94\n",
            "    SM Frequency                                                             cycle/usecond                         594.49\n",
            "    Elapsed Cycles                                                                   cycle                          3,064\n",
            "    Memory [%]                                                                           %                           1.16\n",
            "    SOL DRAM                                                                             %                           1.16\n",
            "    Duration                                                                       usecond                           5.15\n",
            "    SOL L1/TEX Cache                                                                     %                          27.62\n",
            "    SOL L2 Cache                                                                         %                           0.83\n",
            "    SM Active Cycles                                                                 cycle                          46.35\n",
            "    SM [%]                                                                               %                           0.42\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                      1,024\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                          1,024\n",
            "    Waves Per SM                                                                                                     0.03\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources.            \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                              4\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                              1\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          79.52\n",
            "    Achieved Active Warps Per SM                                                      warp                          25.45\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex1 32768"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzzEkgCum-b8",
        "outputId": "db22ab6a-3422-457a-f187-16ebe0e6030c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 32768\n",
            "cost Host->Device: 0.000195 - cost Device->Host: 0.000215\n",
            "CPU cost: 0.000247 - GPU cost: 0.000027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex1 65536"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlJUwosUmua-",
        "outputId": "8c30124b-4c06-438f-b34a-988a0f99a4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 65536\n",
            "cost Host->Device: 0.000353 - cost Device->Host: 0.000429\n",
            "CPU cost: 0.000460 - GPU cost: 0.000030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex1 131070"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF-iISqy7O2E",
        "outputId": "bce35f50-bdd4-4d41-c4e7-c2873107cf32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 131070\n",
            "cost Host->Device: 0.000679 - cost Device->Host: 0.000825\n",
            "CPU cost: 0.000914 - GPU cost: 0.000039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex1 262144"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ5is-TImTOm",
        "outputId": "d17bb38c-fac4-410a-a6bd-6f68b92cb8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 262144\n",
            "cost Host->Device: 0.001117 - cost Device->Host: 0.001524\n",
            "CPU cost: 0.001756 - GPU cost: 0.000046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex1 524288"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsLxFIIumS5s",
        "outputId": "339c7aa8-5d6f-4ef0-aa55-83799ed5f6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 524288\n",
            "cost Host->Device: 0.002047 - cost Device->Host: 0.002832\n",
            "CPU cost: 0.003772 - GPU cost: 0.000070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pyu6OlvOmTYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5J2sQ38OmTen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda-11/bin/nv-nsight-cu-cli ./lab3_ex1 131070"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjYKZylS8s2q",
        "outputId": "05fb272c-a11e-47a1-b5ed-816156bd265f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 131070\n",
            "==PROF== Connected to process 511 (/content/lab3_ex1)\n",
            "==PROF== Profiling \"vecAdd\" - 1: 0%....50%....100% - 8 passes\n",
            "cost Host->Device: 0.000738 - cost Device->Host: 0.000855\n",
            "CPU cost: 0.000898 - GPU cost: 0.719693\n",
            "==PROF== Disconnected from process 511\n",
            "[511] lab3_ex1@127.0.0.1\n",
            "  vecAdd(double*, double*, double*, int), 2022-Dec-15 11:54:22, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.88\n",
            "    SM Frequency                                                             cycle/usecond                         585.85\n",
            "    Elapsed Cycles                                                                   cycle                          8,407\n",
            "    Memory [%]                                                                           %                          52.05\n",
            "    SOL DRAM                                                                             %                          52.05\n",
            "    Duration                                                                       usecond                          14.34\n",
            "    SOL L1/TEX Cache                                                                     %                          26.58\n",
            "    SOL L2 Cache                                                                         %                          27.61\n",
            "    SM Active Cycles                                                                 cycle                       6,163.25\n",
            "    SM [%]                                                                               %                          19.51\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n",
            "          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n",
            "          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                      1,024\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                         128\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        131,072\n",
            "    Waves Per SM                                                                                                     3.20\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    \n",
            "          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       \n",
            "          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 8 thread blocks.    \n",
            "          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   \n",
            "          up to 25.0% of the total kernel runtime with a lower occupancy of 22.5%. Try launching a grid with no         \n",
            "          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  \n",
            "          a grid.                                                                                                       \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                              4\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                              1\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          77.54\n",
            "    Achieved Active Warps Per SM                                                      warp                          24.81\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./lab3_ex1 134217728"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G06yPwY87PR",
        "outputId": "ee1243ca-cb42-4198-cab7-89b089c9d111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 134217728\n",
            "tcmalloc: large alloc 1073741824 bytes == 0x56488a114000 @  0x7fc8a578f1e7 0x564888a076d1 0x7fc8a4422c87 0x564888a0748a\n",
            "==523== NVPROF is profiling process 523, command: ./lab3_ex1 134217728\n",
            "cost Host->Device: 0.463643 - cost Device->Host: 0.729260\n",
            "CPU cost: 0.930963 - GPU cost: 0.012200\n",
            "==523== Profiling application: ./lab3_ex1 134217728\n",
            "==523== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   60.50%  727.87ms         1  727.87ms  727.87ms  727.87ms  [CUDA memcpy DtoH]\n",
            "                   38.49%  463.12ms         2  231.56ms  230.84ms  232.28ms  [CUDA memcpy HtoD]\n",
            "                    1.01%  12.145ms         1  12.145ms  12.145ms  12.145ms  vecAdd(double*, double*, double*, int)\n",
            "      API calls:   81.46%  1.19279s         3  397.60ms  231.09ms  729.25ms  cudaMemcpy\n",
            "                   17.23%  252.27ms         3  84.091ms  1.1112ms  250.05ms  cudaMalloc\n",
            "                    0.84%  12.241ms         3  4.0803ms  9.8560us  12.150ms  cudaDeviceSynchronize\n",
            "                    0.43%  6.2507ms         3  2.0836ms  1.1501ms  3.0070ms  cudaFree\n",
            "                    0.02%  362.12us         1  362.12us  362.12us  362.12us  cuDeviceTotalMem\n",
            "                    0.01%  170.57us       101  1.6880us     140ns  74.014us  cuDeviceGetAttribute\n",
            "                    0.00%  46.672us         1  46.672us  46.672us  46.672us  cudaLaunchKernel\n",
            "                    0.00%  37.437us         1  37.437us  37.437us  37.437us  cuDeviceGetName\n",
            "                    0.00%  6.7500us         1  6.7500us  6.7500us  6.7500us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.9060us         3     635ns     245ns  1.2230us  cuDeviceGetCount\n",
            "                    0.00%  1.2310us         2     615ns     221ns  1.0100us  cuDeviceGet\n",
            "                    0.00%     287ns         1     287ns     287ns     287ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y1tq07k5_r-n"
      }
    }
  ]
}