{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tFKmlIc_vps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6711c6-2970-48ee-ff14-625e86a3d973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting lab3_ex2.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile lab3_ex2.cu\n",
        "\n",
        "#include  <stdio.h>\n",
        "#include  <sys/time.h>\n",
        "#include  <stdlib.h>\n",
        "#include  <random>\n",
        "//#define DataType double\n",
        "#define DataType float\n",
        "double cpuSeconds() {\n",
        "  struct timeval tp;\n",
        "  gettimeofday(&tp,NULL);\n",
        "  return ((double)tp.tv_sec + (double)tp.tv_usec * 1.e-6);\n",
        "}\n",
        "// Compute C = A * B\n",
        "__global__ void gemm(DataType *A, DataType *B, DataType *C, int numARows,\n",
        "                      int numAColumns, int numBRows, int numBColumns){\n",
        "  //@@ Insert code to implement matrix multiplication here\n",
        "  int row = blockDim.y * blockIdx.y + threadIdx.y;\n",
        "  int col = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "  if ((col >= numBColumns) || (row >= numARows)) return;\n",
        "  double sum=0.0;\n",
        "  for (int k = 0; k < numAColumns; k++) {\n",
        "        sum += A[row*numAColumns + k] * B[k*numBColumns + col];\n",
        "    }\n",
        "    C[row*numBColumns+col]=sum;\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "  \n",
        "  DataType *hostA; // The A matrix\n",
        "  DataType *hostB; // The B matrix\n",
        "  DataType *hostC; // The output C matrix\n",
        "  DataType *resultRef; // The reference result\n",
        "  DataType *deviceA;\n",
        "  DataType *deviceB;\n",
        "  DataType *deviceC;\n",
        "  int numARows;    // number of rows in the matrix A\n",
        "  int numAColumns; // number of columns in the matrix A\n",
        "  int numBRows;    // number of rows in the matrix B\n",
        "  int numBColumns; // number of columns in the matrix B\n",
        "  int numCRows;\n",
        "  int numCColumns;\n",
        "\n",
        "  //@@ Insert code below to read in numARows, numAColumns, numBColumns from args\n",
        "   numARows = atoi(argv[1]);\n",
        "   numAColumns = atoi(argv[2]);\n",
        "   numBRows = atoi(argv[3]);\n",
        "   numBColumns = atoi(argv[4]);\n",
        "   numCRows = numARows;\n",
        "   numCColumns = numBColumns;\n",
        "  printf(\"Input matrix dim (%d x %d) (%d x %d) (%d x %d)\\n\", numARows, numAColumns, numBRows, numBColumns, numCRows, numCColumns);\n",
        "  if (numAColumns != numBRows) {\n",
        "        printf(\"ERROR: the matrix could not be multiplied\" );\n",
        "        return 0;\n",
        "    }\n",
        "\n",
        "  //@@ Insert code below to allocate Host memory for input and output\n",
        "  hostA = (DataType*)malloc(numARows * numAColumns * sizeof(DataType));\n",
        "  hostB = (DataType*)malloc(numBRows * numBColumns * sizeof(DataType));\n",
        "  hostC = (DataType*)malloc(numCRows * numCColumns * sizeof(DataType));\n",
        "  \n",
        "  //@@ Insert code below to initialize hostA and hostB to random numbers, and create reference result in CPU\n",
        "  std::uniform_real_distribution<DataType> distribution(0.0, 1.0);\n",
        "  std::default_random_engine gen(1145);\n",
        "  for (int i = 0; i < numARows; ++i) {\n",
        "        for (int j = 0; j < numAColumns; ++j) {\n",
        "            DataType randomNumber = distribution(gen);\n",
        "            hostA[i*numAColumns + j] = randomNumber;\n",
        "            \n",
        "        }\n",
        "    }\n",
        "    for (int i = 0; i < numBRows; ++i) {\n",
        "        for (int j = 0; j < numBColumns; ++j) {\n",
        "            DataType randomNumber = distribution(gen); \n",
        "            hostB[i*numBColumns + j] = randomNumber;\n",
        "            \n",
        "        }\n",
        "    }\n",
        "  resultRef = (DataType*)malloc(numCRows * numCColumns * sizeof(DataType));\n",
        "  for(int i=0; i<numARows;i++){\n",
        "    for(int j=0;j<numBColumns;j++){\n",
        "       resultRef[i*numBColumns+j]=0.0;\n",
        "      for(int k=0;k<numBRows;k++){\n",
        "        resultRef[i*numBColumns+j]+=hostA[k+numBRows*i]*hostB[j+numBColumns*k];\n",
        "      }\n",
        "      \n",
        "    }\n",
        "  }\n",
        "  //@@ Insert code below to allocate GPU memory here\n",
        "  cudaMalloc(&deviceA, numARows * numAColumns * sizeof(DataType));\n",
        "  cudaMalloc(&deviceB, numBRows * numBColumns * sizeof(DataType));\n",
        "  cudaMalloc(&deviceC, numCRows * numCColumns * sizeof(DataType));\n",
        "  \n",
        "  //@@ Insert code to below to Copy memory to the GPU here\n",
        "  double start = cpuSeconds();\n",
        "  cudaMemcpy(deviceA, hostA, numARows * numAColumns * sizeof(DataType), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(deviceB, hostB, numBRows * numBColumns * sizeof(DataType), cudaMemcpyHostToDevice);\n",
        "  double duration = cpuSeconds() - start;\n",
        "  printf(\"float value\\nHost -> Device: %f\\n\", duration);\n",
        "  \n",
        "  int Dg_x=(numCColumns+32-1)/32;\n",
        "  int Dg_y=(numCRows+32-1)/32;\n",
        "  //@@ Initialize the grid and block dimensions here\n",
        "  dim3 Dg(Dg_x,Dg_y,1);\n",
        "  dim3 Db(32,32,1);\n",
        "\n",
        "  //@@ Launch the GPU Kernel here\n",
        "  start = cpuSeconds();\n",
        "  gemm<<<Dg,Db>>>(deviceA,deviceB,deviceC,numARows,numAColumns,numBRows,numBColumns);\n",
        "  cudaDeviceSynchronize();\n",
        "  duration = cpuSeconds() - start;\n",
        "  printf(\"Kernel: %f\\n\", duration);\n",
        "\n",
        "  //@@ Copy the GPU memory back to the CPU here\n",
        "  start = cpuSeconds();\n",
        "  cudaMemcpy(hostC, deviceC,  numCRows * numCColumns *sizeof(DataType), cudaMemcpyDeviceToHost);\n",
        "  cudaDeviceSynchronize();\n",
        "  duration = cpuSeconds() - start;\n",
        "  printf(\"Device->Host: %f\\n\", duration);\n",
        "  //@@ Insert code below to compare the output with the reference\n",
        "  for(int i=0;i<numCRows * numCColumns;i++){\n",
        "    if(hostC[i]!=resultRef[i] && abs(resultRef[i]-hostC[i])>0.0001 ){\n",
        "      printf(\"error %f - %f\\n\",hostC[i],resultRef[i]);\n",
        "      return -1;\n",
        "    }\n",
        "  }\n",
        "  printf(\"the commdan is correct\");\n",
        "  //@@ Free the GPU memory here\n",
        "  cudaFree(deviceA);\n",
        "  cudaFree(deviceB);\n",
        "  cudaFree(deviceC);\n",
        "\n",
        "  //@@ Free the CPU memory here\n",
        "  free(hostA);\n",
        "  free(hostB);\n",
        "  free(hostC);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 ./lab3_ex2.cu -o lab3_ex2"
      ],
      "metadata": {
        "id": "R3ZSUq4nXzFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 128 128 128 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z7gTS22ePHS",
        "outputId": "3439a5ad-3280-4c80-d76c-651638c689b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (128 x 128) (128 x 128) (128 x 128)\n",
            "Host -> Device: 0.000099\n",
            "Kernel: 0.000147\n",
            "Device->Host: 0.000134\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 256 128 128 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5kjggNuJgwa",
        "outputId": "97260a9b-1522-4b62-cf3f-beadf50679b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (256 x 128) (128 x 256) (256 x 256)\n",
            "Host -> Device: 0.000167\n",
            "Kernel: 0.000267\n",
            "Device->Host: 0.000380\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 512 128 128 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnZ2pj6QJm7N",
        "outputId": "289ab153-ab39-4aa7-d852-3a286a90c182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (512 x 128) (128 x 512) (512 x 512)\n",
            "Host -> Device: 0.000291\n",
            "Kernel: 0.000869\n",
            "Device->Host: 0.001448\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 1024 128 128 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AndjdCOQJpi6",
        "outputId": "7d75ac4b-5a3f-4042-d3ef-404391f16be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (1024 x 128) (128 x 1024) (1024 x 1024)\n",
            "Host -> Device: 0.000625\n",
            "Kernel: 0.003164\n",
            "Device->Host: 0.005803\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 2048 128 128 2048"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9227v1TGJtZG",
        "outputId": "fa5329a7-14af-4fea-dfce-508816f06526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (2048 x 128) (128 x 2048) (2048 x 2048)\n",
            "Host -> Device: 0.001006\n",
            "Kernel: 0.012258\n",
            "Device->Host: 0.021445\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 4096 128 128 4096"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcvHX5WFJ3oY",
        "outputId": "9ddd1d21-9dc6-4687-fc69-4dc80f18e138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (4096 x 128) (128 x 4096) (4096 x 4096)\n",
            "Host -> Device: 0.001966\n",
            "Kernel: 0.048724\n",
            "Device->Host: 0.086303\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 128 128 128 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRcm7gulKRsi",
        "outputId": "3ec95c3c-1791-4f1c-c293-01875c729da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (128 x 128) (128 x 128) (128 x 128)\n",
            "float value\n",
            "Host -> Device: 0.000077\n",
            "Kernel: 0.000289\n",
            "Device->Host: 0.000076\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 256 128 128 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grT4xdVZKRvU",
        "outputId": "83ad3e3e-a6f1-48ac-aae6-2f094a622b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (256 x 128) (128 x 256) (256 x 256)\n",
            "float value\n",
            "Host -> Device: 0.000095\n",
            "Kernel: 0.000548\n",
            "Device->Host: 0.000201\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 512 128 128 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_79KKVVKRyO",
        "outputId": "8b9a55e0-cf2e-4aef-ea15-140e2d13f85e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (512 x 128) (128 x 512) (512 x 512)\n",
            "float value\n",
            "Host -> Device: 0.000170\n",
            "Kernel: 0.001849\n",
            "Device->Host: 0.000734\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 1024 128 128 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46oJH76bKR09",
        "outputId": "50b286ba-54b3-4f4e-e8a2-f7cce4dc5331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (1024 x 128) (128 x 1024) (1024 x 1024)\n",
            "float value\n",
            "Host -> Device: 0.000319\n",
            "Kernel: 0.006803\n",
            "Device->Host: 0.002786\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 2048 128 128 2048"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jlps-F6KR39",
        "outputId": "476a8ad7-2fbd-44cb-df63-e4bef08b5bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (2048 x 128) (128 x 2048) (2048 x 2048)\n",
            "float value\n",
            "Host -> Device: 0.000555\n",
            "Kernel: 0.026844\n",
            "Device->Host: 0.010799\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 4096 128 128 4096"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zeu8QaymKR-F",
        "outputId": "50e8a286-ca9f-45f4-d00a-06fb0774dcaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (4096 x 128) (128 x 4096) (4096 x 4096)\n",
            "float value\n",
            "Host -> Device: 0.001014\n",
            "Kernel: 0.106622\n",
            "Device->Host: 0.043184\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda-11/bin/nv-nsight-cu-cli ./lab3_ex2 128 128 128 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6DW0_d88AWV",
        "outputId": "8a977e59-f2d0-42f5-ad66-29acef1702c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (128 x 128) (128 x 128) (128 x 128)\n",
            "==PROF== Connected to process 319 (/content/lab3_ex2)\n",
            "==PROF== Profiling \"gemm\" - 1: 0%....50%....100% - 8 passes\n",
            "the commdan is correct==PROF== Disconnected from process 319\n",
            "[319] lab3_ex2@127.0.0.1\n",
            "  gemm(double*, double*, double*, int, int, int, int), 2022-Dec-15 13:19:08, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.99\n",
            "    SM Frequency                                                             cycle/usecond                         584.02\n",
            "    Elapsed Cycles                                                                   cycle                         71,433\n",
            "    Memory [%]                                                                           %                           9.28\n",
            "    SOL DRAM                                                                             %                           1.16\n",
            "    Duration                                                                       usecond                         122.30\n",
            "    SOL L1/TEX Cache                                                                     %                          23.68\n",
            "    SOL L2 Cache                                                                         %                           1.20\n",
            "    SM Active Cycles                                                                 cycle                      28,001.42\n",
            "    SM [%]                                                                               %                          36.70\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.4 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                      1,024\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                          16\n",
            "    Registers Per Thread                                                   register/thread                             62\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                         16,384\n",
            "    Waves Per SM                                                                                                     0.40\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 40             \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources.            \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                              1\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                              1\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          96.28\n",
            "    Achieved Active Warps Per SM                                                      warp                          30.81\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./lab3_ex2 128 128 128 128"
      ],
      "metadata": {
        "id": "q-TQ2Fw0eZS5",
        "outputId": "e1858df3-5feb-4ae0-8354-5fba44e89be9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (128 x 128) (128 x 128) (128 x 128)\n",
            "==264== NVPROF is profiling process 264, command: ./lab3_ex2 128 128 128 128\n",
            "the commdan is correct==264== Profiling application: ./lab3_ex2 128 128 128 128\n",
            "==264== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   75.67%  123.61us         1  123.61us  123.61us  123.61us  gemm(double*, double*, double*, int, int, int, int)\n",
            "                   16.91%  27.616us         2  13.808us  13.600us  14.016us  [CUDA memcpy HtoD]\n",
            "                    7.42%  12.128us         1  12.128us  12.128us  12.128us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.63%  298.32ms         3  99.439ms  3.0130us  298.31ms  cudaMalloc\n",
            "                    0.12%  359.20us         1  359.20us  359.20us  359.20us  cuDeviceTotalMem\n",
            "                    0.09%  255.00us         3  84.999us  53.853us  129.81us  cudaMemcpy\n",
            "                    0.05%  146.14us         3  48.712us  3.9110us  129.52us  cudaFree\n",
            "                    0.05%  139.59us       101  1.3820us     127ns  59.125us  cuDeviceGetAttribute\n",
            "                    0.04%  126.69us         1  126.69us  126.69us  126.69us  cudaDeviceSynchronize\n",
            "                    0.01%  32.542us         1  32.542us  32.542us  32.542us  cudaLaunchKernel\n",
            "                    0.01%  28.027us         1  28.027us  28.027us  28.027us  cuDeviceGetName\n",
            "                    0.00%  7.0940us         1  7.0940us  7.0940us  7.0940us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0150us         3     671ns     193ns  1.2960us  cuDeviceGetCount\n",
            "                    0.00%  1.6130us         2     806ns     325ns  1.2880us  cuDeviceGet\n",
            "                    0.00%     231ns         1     231ns     231ns     231ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab3_ex2 511 1023 1023 4094"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlyJpZWEY3eN",
        "outputId": "3e062a8b-2b78-4742-f993-cca07e9b13c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (511 x 1023) (1023 x 4094) (511 x 4094)\n",
            "the commdan is correct"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda-11/bin/nv-nsight-cu-cli ./lab3_ex2 511 1023 1023 4094"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPj89-k880-T",
        "outputId": "514c66c8-bcc2-46e7-f7fd-aeb95ceeb988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (511 x 1023) (1023 x 4094) (511 x 4094)\n",
            "==PROF== Connected to process 347 (/content/lab3_ex2)\n",
            "==PROF== Profiling \"gemm\" - 1: 0%....50%....100% - 8 passes\n",
            "the commdan is correct==PROF== Disconnected from process 347\n",
            "[347] lab3_ex2@127.0.0.1\n",
            "  gemm(double*, double*, double*, int, int, int, int), 2022-Dec-15 13:23:05, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           5.00\n",
            "    SM Frequency                                                             cycle/usecond                         584.99\n",
            "    Elapsed Cycles                                                                   cycle                     28,127,262\n",
            "    Memory [%]                                                                           %                          25.29\n",
            "    SOL DRAM                                                                             %                           5.41\n",
            "    Duration                                                                       msecond                          48.08\n",
            "    SOL L1/TEX Cache                                                                     %                          50.59\n",
            "    SOL L2 Cache                                                                         %                           3.03\n",
            "    SM Active Cycles                                                                 cycle                  27,743,821.38\n",
            "    SM [%]                                                                               %                          95.16\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    OK    The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing workloads in the Compute Workload Analysis section.                                        \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                      1,024\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       2,048\n",
            "    Registers Per Thread                                                   register/thread                             62\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                      2,097,152\n",
            "    Waves Per SM                                                                                                    51.20\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                              1\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                              1\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          98.56\n",
            "    Achieved Active Warps Per SM                                                      warp                          31.54\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./lab3_ex2 2048 1536 1536 4096"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaNBWHXIZ4MZ",
        "outputId": "cebe5937-4cbb-4485-8cf1-2dedfe649209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (2048 x 1536) (1536 x 4096) (2048 x 4096)\n",
            "==284== NVPROF is profiling process 284, command: ./lab3_ex2 2048 1536 1536 4096\n",
            "the commdan is correct==284== Profiling application: ./lab3_ex2 2048 1536 1536 4096\n",
            "==284== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   79.03%  222.78ms         1  222.78ms  222.78ms  222.78ms  gemm(double*, double*, double*, int, int, int, int)\n",
            "                   15.00%  42.291ms         1  42.291ms  42.291ms  42.291ms  [CUDA memcpy DtoH]\n",
            "                    5.96%  16.804ms         2  8.4022ms  6.3323ms  10.472ms  [CUDA memcpy HtoD]\n",
            "      API calls:   51.89%  309.06ms         3  103.02ms  136.62us  308.78ms  cudaMalloc\n",
            "                   37.42%  222.84ms         1  222.84ms  222.84ms  222.84ms  cudaDeviceSynchronize\n",
            "                   10.21%  60.822ms         3  20.274ms  6.5959ms  43.537ms  cudaMemcpy\n",
            "                    0.36%  2.1447ms         3  714.90us  231.90us  1.1422ms  cudaFree\n",
            "                    0.08%  455.43us         1  455.43us  455.43us  455.43us  cuDeviceTotalMem\n",
            "                    0.03%  182.12us       101  1.8030us     151ns  85.621us  cuDeviceGetAttribute\n",
            "                    0.01%  35.506us         1  35.506us  35.506us  35.506us  cudaLaunchKernel\n",
            "                    0.00%  28.220us         1  28.220us  28.220us  28.220us  cuDeviceGetName\n",
            "                    0.00%  8.3810us         1  8.3810us  8.3810us  8.3810us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.3600us         3     453ns     189ns     823ns  cuDeviceGetCount\n",
            "                    0.00%  1.1030us         2     551ns     344ns     759ns  cuDeviceGet\n",
            "                    0.00%     285ns         1     285ns     285ns     285ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    }
  ]
}
